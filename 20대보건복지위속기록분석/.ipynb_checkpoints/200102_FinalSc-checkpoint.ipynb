{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEETING_NAME\n",
      "제343회(2016.06.07-2016.07.06)    3100\n",
      "제344회(2016.07.19-2016.07.27)     517\n",
      "제345회(2016.08.16-2016.08.31)     750\n",
      "제346회(2016.09.01-2016.12.09)     853\n",
      "제347회(2016.12.12-2016.12.31)    1178\n",
      "제348회(2017.01.09-2017.01.20)     128\n",
      "제349회(2017.02.01-2017.03.02)    2157\n",
      "제350회(2017.03.03-2017.04.01)     159\n",
      "제352회(2017.07.04-2017.07.22)    3035\n",
      "제353회(2017.08.18-2017.08.31)    1003\n",
      "제354회(2017.09.01-2017.12.09)    1384\n",
      "제355회(2017.12.11-2017.12.29)     394\n",
      "제356회(2018.01.30-2018.02.28)    1221\n",
      "제362회(2018.07.13-2018.07.26)    1882\n",
      "제363회(2018.08.16-2018.08.31)    1287\n",
      "제364회(2018.09.01-2018.12.09)    1253\n",
      "dtype: int64\n",
      "PARTY\n",
      "국민의당      1209\n",
      "더불어민주당    5615\n",
      "무소속        656\n",
      "바른정당       184\n",
      "자유한국당     2824\n",
      "dtype: int64\n",
      "SPEAKER\n",
      "강석진 위원                 242\n",
      "건강보험심사평가원개발상임이사 황의동      4\n",
      "건강보험심사평가원상임감사 서정숙        2\n",
      "건강보험심사평가원장 김승택          91\n",
      "건강보험심사평가원장 손명세          68\n",
      "                      ... \n",
      "천정배 위원                 282\n",
      "천정배 의원                   1\n",
      "최교일 의원                   1\n",
      "최도자 위원                 428\n",
      "최도자 의원                   1\n",
      "Length: 143, dtype: int64\n",
      "ALL (20301, 7)\n",
      "fdata (656, 7)\n"
     ]
    }
   ],
   "source": [
    "from Mylib import *\n",
    "from os import *\n",
    "\n",
    "now = datetime.now()\n",
    "tstmp = now.strftime(\"%y%m%d;%H%M\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Loading\n",
    "# =============================================================================\n",
    "# Loading Tokens\n",
    "noun_tokened = pd.read_pickle('./result/HWC_noun_token.pkl')\n",
    "print(noun_tokened.groupby(['MEETING_NAME']).size())\n",
    "print(noun_tokened.groupby(['PARTY']).size())\n",
    "print(noun_tokened.groupby(['SPEAKER']).size())\n",
    "# all_tokened = pd.read_pickle('./result/HWC_all_token.pkl')\n",
    "\n",
    "### 2. Data cleansing\n",
    "noun_tokened.loc[noun_tokened['SPEAKER']=='박인숙 위원', 'PARTY']='자유한국당'\n",
    "# Domain Knowledge based Filtering\n",
    "Coding = pd.read_csv('./data/단어집.csv')\n",
    "DKF = Coding[Coding['code'] == 1]['단어']\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Custom criterions \n",
    "# =============================================================================\n",
    "addf = [\n",
    "    '국민', # 상투어\n",
    "    '위원장', '위원', '의원', '장관', # 직책\n",
    "    '질의', # 회의 진행용어\n",
    "    '보건', '복지' # 보건복지위원회!\n",
    "]\n",
    "\n",
    "정당 = ['무소속']\n",
    "#정당 = ['바른정당','자유한국당'] # 보수\n",
    "#정당 = ['국민의당','더불어민주당','무소속'] # 진보\n",
    "#회차 = [343, 350] # 정권교체 전\n",
    "#회차 = [352, 364] # 정권교체 후\n",
    "\n",
    "#발언자 = ['강석진','천정배'] # 이름검색 필요!\n",
    "\n",
    "# Data Preparation\n",
    "DKF = DKF.append(pd.Series(addf))\n",
    "noun_tokened['token'] = [list(set(d) - set(DKF)) for d in noun_tokened['token']]\n",
    "\n",
    "#fdata = noun_tokened # All data\n",
    "\n",
    "fdata = prpr_data(noun_tokened).by_party(정당).get_data()\n",
    "# fdata = prpr_data(noun_tokened).by_meeting(회차).get_data()\n",
    "\n",
    "tokens = fdata['token'].copy()\n",
    "\n",
    "print('ALL', noun_tokened.shape)\n",
    "print('fdata', fdata.shape)\n",
    "makedirs('result/'+tstmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 1 ...\n",
      "Unreachable objects : 4836\n",
      "Remaining Garbage : []\n",
      "Collecting 2 ...\n",
      "Unreachable objects : 0\n",
      "Remaining Garbage : []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 656/656 [00:00<00:00, 32821.55it/s]\n",
      "  6%|████▉                                                                              | 3/50 [00:15<03:52,  4.95s/it]"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. Word Cloud\n",
    "# =============================================================================\n",
    "wordsvec = [item for sublist in tokens for item in sublist]\n",
    "wordsvec = pd.Series(wordsvec)\n",
    "#print(wordsvec.loc[0:5])\n",
    "wcld = WordCloud(width=600, height=600, background_color='white', \n",
    "                 font_path = './NanumBarunGothic.ttf')\n",
    "cnt = Counter(wordsvec.tolist())\n",
    "wcld = wcld.generate_from_frequencies(cnt)\n",
    "#wcld.to_image()\n",
    "wcld.to_file('./result/'+tstmp+'/WC.png')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Corpus Normalization & TF-IDF\n",
    "# =============================================================================\n",
    "gcol()\n",
    "for j in tqdm(pd.DataFrame.keys(tokens)):\n",
    "    tokens[j] = ' '.join(tokens[j])\n",
    "ncorpus = tokens[~tokens.isin(addf)].tolist() # Normalized corpus.\n",
    "#print('Normalized corpus example : %s' %ncorpus[1:3])\n",
    "\n",
    "# TF-IDF\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "tv_matrix = tv.fit_transform(ncorpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "TFIDF = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
    "TFIDF.to_csv('./result/'+tstmp+'/TFIDF.csv')\n",
    "#TFIDF.head(10) # vocab에 대한 TFIDF score\n",
    "#TFIDF.shape # vocab에 대한 TFIDF score\n",
    "\n",
    "# =============================================================================\n",
    "# 5. LDA\n",
    "# =============================================================================\n",
    "a, b = 1, 50\n",
    "perpl, coher = [], []\n",
    "for top in tqdm(range(a,b+1)) :\n",
    "    _,p,c = doLDA(fdata['token'], top, 'ALL', False)\n",
    "    perpl.append(p)\n",
    "    coher.append(c)\n",
    "    del _,p,c\n",
    "\n",
    "# Tuning Process\n",
    "plt.figure(figsize=(10, 14))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.grid(True)\n",
    "plt.plot(range(a,b+1), perpl, '-.', label='ALL')\n",
    "plt.title('Perplexities, '+'Opt. ntopics='+str(range(a,b+1)[np.argmin(perpl)]))\n",
    "plt.ylabel('log perplexities')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.grid(True)\n",
    "plt.plot(range(a,b+1), coher, '-.', label='ALL')\n",
    "plt.title('Coherences, '+'Opt. ntopics='+str(range(a,b+1)[np.argmax(coher)]))\n",
    "plt.xlabel('n Topics')\n",
    "plt.ylabel('Coherence')\n",
    "\n",
    "plt.savefig('./result/'+tstmp+'/perpcoh.png')\n",
    "\n",
    "# Final results with pyLDAvis\n",
    "#flda = doLDA(fdata['token'], range(a,b+1)[np.argmin(perpl)], \n",
    "#             './result/'+tstmp, True)\n",
    "optcoh = range(a,b+1)[np.argmax(coher)]\n",
    "flda = doLDA(fdata['token'], optcoh, './result/'+tstmp, True)\n",
    "fldatops = flda[0].print_topics(num_words=10)\n",
    "fldatops = pd.DataFrame(fldatops, columns=['Topic','Representation'])\n",
    "fldatops.to_excel('./result/'+tstmp+'/fldatops.xlsx')\n",
    "path = './result/'+tstmp+'/'+str(optcoh)+'_pyLDAvis.pickle'\n",
    "pyLDAvis = pd.read_pickle(path)\n",
    "pyLDAvis\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Networkds\n",
    "# =============================================================================\n",
    "\n",
    "# Gathering keywords\n",
    "ftopkw = np.unique([j[0] for \n",
    "                    i in range(flda[0].num_topics) for\n",
    "                    j in flda[0].get_topic_terms(i)])\n",
    "ftopkw = [flda[0].id2word[i] for i in ftopkw]\n",
    "\n",
    "# Gephi (coocurrence)\n",
    "gcol()\n",
    "norm_corp = fdata['token'].copy()\n",
    "for j in tqdm(range(len(norm_corp))):\n",
    "    norm_corp.iloc[j] = ' '.join(intersection(norm_corp.iloc[j],ftopkw))\n",
    "norm_corp = norm_corp.tolist() # Normalized corpus.\n",
    "\n",
    "print('Normalized corpus example : %s' %norm_corp[1:3])\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,1))\n",
    "X = cv.fit_transform(norm_corp)\n",
    "\n",
    "Xc = (X.T * X) # This is the matrix manipulation step\n",
    "Xc.setdiag(0) # We set the diagonals to be zeroes as it's pointless to be 1\n",
    "names = cv.get_feature_names() # This are the entity names (i.e. keywords)\n",
    "df = pd.DataFrame(data = Xc.toarray(), columns = names, index = names)\n",
    "\n",
    "df.to_csv('./result/'+tstmp+'/coocurr_for_gephi.csv', sep = ',', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(pyLDAvis[1]).to_csv('./result/'+tstmp+'/pyLDAvis.csv', encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
